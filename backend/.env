# Vocalis backend configuration

# API Endpoints
LLM_API_ENDPOINT=http://127.0.0.1:1234/v1/chat/completions  # Place your local LLM API endpoint here (default is LM Studio)
#TTS_API_ENDPOINT=http://localhost:5005/v1/audio/speech  # Place your local TTS API endpoint here (default is Orpheus-FASTAPI native python launcher) - If you're using Orpheus-FASTAPI Docker Container versus native python launcher, replace "localhost" with "127.0.0.1:5005"

TTS_API_ENDPOINT=http://127.0.0.1:8880/v1/audio/speech

# Whisper Model Configuration
WHISPER_MODEL=small.en  # Options: tiny.en, base.en, small.en, medium.en, large

# TTS Configuration
TTS_MODEL=tts-1 
TTS_VOICE=af_bella 
TTS_FORMAT=wav        # Format for TTS output (wav, mp3, opus, flac)

# WebSocket Server Configuration
WEBSOCKET_HOST=0.0.0.0
WEBSOCKET_PORT=8000


# Audio Processing
VAD_THRESHOLD=0.3         # Voice activity detection threshold (0.0-1.0)
VAD_BUFFER_SIZE=200         # Buffer size in milliseconds
AUDIO_SAMPLE_RATE=16000    # Sample rate in Hz
 
# Vision Processing
 ENABLE_VISION_MODEL=False  # Enable vision model (True/False)